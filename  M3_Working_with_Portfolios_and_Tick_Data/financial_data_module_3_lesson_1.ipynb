{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_djuWRzMXkq"
   },
   "source": [
    "MODULE 3 | LESSON 1\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **Portfolio Returns and Variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSW9-996Miww"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** | 2h  |\n",
    "|**Prior Knowledge** | python, returns, variance, diversification  |\n",
    "|**Keywords** | returns, logarithm, percent, matrix notation, distribution, annualize, geometric mean, arithmetic mean, portfolio variance, correlation, weights|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6JWZbCYFhmI"
   },
   "source": [
    "*In this lesson, we'll explore various types of returns, methods for annualizing returns, and techniques for calculating portfolio variance. The lesson aims to provide a comprehensive understanding of how to measure and interpret portfolio performance, considering both returns and risk.*\n",
    "\n",
    "*We'll start by examining different types of returns, including percentage and logarithmic returns, and discuss their properties and appropriate use cases. We'll then move on to methods for annualizing returns, which is crucial for comparing investments over different time periods. The lesson will also cover portfolio variance calculation, exploring how individual asset variances, weights, and correlations contribute to overall portfolio risk.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tsZsezGth8R"
   },
   "source": [
    "## **1. Portfolio Returns**\n",
    "\n",
    "In Financial Markets, you were introduced to asset and portfolio returns. In Financial Data, we will explore both one-dimensional and multi-dimensional datasets. Therefore, matrix representations in linear algebra will be the main gateway for dealing with this subject. This makes sense because a portfolio consists of $n$ assets with returns $r_{i}, i \\in \\{1,n\\}$ that can be arranged in a matrix $r$ where each column is represented by an asset. In this context, $r_{i}$ are vectors, and **in the absence of perfect multicollinearity**, it is true that $\\text{dim}(\\text{span}(r_{i})) = n$.\n",
    "\n",
    "We have already seen that the portfolio returns are:\n",
    "$$\n",
    "r_{p} = \\sum_{1}^{n} w_{i} \\cdot r_{i}\n",
    "$$\n",
    "where\n",
    "* $r_{p}$: portfolio returns\n",
    "* $r_{i}$: asset $i$ returns\n",
    "* $w_{i}$: the weight of asset $i$\n",
    "\n",
    "We should have an additional constraint in mind:\n",
    "$$\n",
    "\\sum_{1}^{n} w_{i} = 1\n",
    "$$\n",
    "\n",
    "and for the purposes of this lesson, we will also assume that shorting is not allowed:\n",
    "$$\n",
    "w_{i} > 0\n",
    "$$\n",
    "\n",
    "Essentially, the return of the portfolio $r_{p}$ is a linear combination of the returns of each asset. By construction, we have that $r_{p} \\in \\text{span}(r_{i})$ for every combination of the weights. That means that the assets that an investor chooses are the basis (in a Linear Algebra context) of the portfolio.\n",
    "\n",
    "Let's now write the portfolio returns in the format we will be using for the rest of the lesson and make the notation clear:\n",
    "\n",
    "$$\n",
    "r_{p} = \\mathbf{r} \\cdot \\mathbf{w}\n",
    "$$\n",
    "\n",
    "with\n",
    "$$ \\mathbf{r} = \\begin{bmatrix} r_1 & r_2 & \\dots & r_n \\end{bmatrix} \\text{ , } \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix}$$\n",
    "\n",
    "and each $r_{i}$ is a vector that represents the returns of each asset across time $t$:\n",
    "$$\n",
    "r_{i} = \\begin{bmatrix} r^i_{t_{1}} \\\\ r^i_{t_{2}} \\\\ \\vdots \\\\ r^i_{t_{m}} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Using $1$ as the sum of all weights has an intuitive explanation: we construct the portfolio by using a percentage of each asset (the weights) until we use the entire amount ($1$ means $100\\%$). What does it mean if we break that constraint?\n",
    "\n",
    "\n",
    "**Exercise 1:**\n",
    "\n",
    "Use your favorite finance textbook and online resources in researching the constraint $\\sum_{1}^{n} w_{i} = 1$. What would it mean for a portfolio if $\\sum_{1}^{n} w_{i} = 2$ ? Answer the same question but now for $\\sum_{1}^{n} w_{i} = 0$. Initiate a discussion in the forum explaining your understanding.\n",
    "\n",
    "Let's download some real-world data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3232,
     "status": "ok",
     "timestamp": 1732130380473,
     "user": {
      "displayName": "Christos Koutkos",
      "userId": "05788829401179389824"
     },
     "user_tz": -120
    },
    "id": "encGrn7iFG7_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1732130380473,
     "user": {
      "displayName": "Christos Koutkos",
      "userId": "05788829401179389824"
     },
     "user_tz": -120
    },
    "id": "CIq0akzMzX81",
    "outputId": "026f4a32-d31f-4407-fb36-768a5b77e372"
   },
   "outputs": [],
   "source": [
    "assets = ['MSFT', 'AAPL', 'AMZN', 'TSLA', 'GOOGL'] # Assets for portfolio\n",
    "w = np.array([0.1, 0.2, 0.1, 0.4, 0.2]) # Weights of each asset\n",
    "\n",
    "asset_prices = yf.download(assets, start='2018-01-01', end='2023-01-01', auto_adjust = False)['Adj Close'] # Downloading daily data\n",
    "asset_prices.index = pd.to_datetime(asset_prices.index) # Setting index as datetime object\n",
    "\n",
    "r = asset_prices.pct_change().dropna() # Calculating daily percent returns\n",
    "\n",
    "r.head() # Each column is r_{i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1732130380474,
     "user": {
      "displayName": "Christos Koutkos",
      "userId": "05788829401179389824"
     },
     "user_tz": -120
    },
    "id": "9V43H4sB0jKV",
    "outputId": "97c6bb37-f828-4f9b-a562-a203e49cf662"
   },
   "outputs": [],
   "source": [
    "r_port = r @ w # Creating portfolio returns\n",
    "r_port.name = 'portfolio_returns'\n",
    "r_port.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mB-08puS-5QJ"
   },
   "source": [
    "## **2 Types of Returns**\n",
    "\n",
    "In the example above, we have essentially presented a way to calculate portfolio daily percent returns. But these returns are not the only ones used in practice:\n",
    "\n",
    "### **2.1 Percentage Returns**\n",
    "\n",
    "Percentage returns, also known as simple returns or arithmetic returns, are commonly used in practice:\n",
    "$$\n",
    "r_{t} = \\frac{p_{t} - p_{t-1}}{p_{t-1}}\n",
    "$$\n",
    "where $p_{t}$ is the price of an asset at time $t$.\n",
    "\n",
    "The biggest bonus of using these returns is that the interpretation is intuitive: they directly represent the percentage change in value. A drawback is that they are not additive across time for one asset even though, for the exact same reason they are commonly used with portfolios.\n",
    "\n",
    "The implications of nonadditivity are simple to show: imagine that you have an asset of price $A$. The first month, the price appreciates by $20\\%$, but at the end of the second month, the price falls by $18\\%$. If the returns were additive, one could just add them $20\\% - 18\\% = 2\\%$ and argue that the total return for the 2 months is $2\\%$. But as we will see in a future section, that is not true.\n",
    "\n",
    "Lastly, the simple returns are not symmetrical. There is no boundary to the upside, yet it is bounded on the downside. Intuitively, the returns of a share cannot be less than $-100\\%$ since the price would become negative. On the upside though, the returns can go really high especially when dealing with large horizons. Another way to showcase this is with the following observation: the price of an asset in three consecutive months is 100 -> 120 -> 100. At the end of the second month, the returns are $20\\%$, but at the end of the third month, they are $-16.6\\%$. If the simple returns were symmetrical, then it would require the same absolute number of returns for going from 100 to 120 and then back to 100.\n",
    "\n",
    "**Exercise 2:**\n",
    "\n",
    "Create a custom function in Python that given a price array (for example `asset_prices['MSFT']`), will return the array of the arithmetic returns. You are not allowed to use any existing Python functions. Cross-check the results with the ones produced by using the `pct_change` pandas function.\n",
    "\n",
    "### **2.2 Logarithmic Returns**\n",
    "\n",
    "The log returns, also known as continuously compounded returns, make use of the natural logarithm in the following way:\n",
    "\n",
    "$$\n",
    "r_{t_\\text{log}} = ln \\left ( \\frac{p_t}{p_{t-1}} \\right )\n",
    "$$\n",
    "\n",
    "In contrast to the percent returns, the log returns are additive for one asset over time:\n",
    "$$\n",
    "r_{(t_{i}-t_{j})_{\\text{log}}} = \\sum_{t = i}^{t = j} r_{t_\\text{log}}\n",
    "$$\n",
    "they are also assumed (not always correctly) to be normally distributed, which is an important assumption for many econometric and stochastic models.\n",
    "\n",
    "The additivity is responsible for the name \"continuously compounded returns\" since now you can just calculate the total returns over large horizons simply by adding the intra-period log returns. This creates a problem though: if we have the individual logarithmic returns of several assets, does it make sense to use them in order to calculate the returns of the portfolio? If adding log returns is a compounding process, are you allowed to add log returns of different assets?\n",
    "\n",
    "The answer to the above questions is **no** and one needs to take extra care in order to use the correct type of return for each use case.\n",
    "\n",
    "Lastly, the log returns are symmetrical. Mathematically, we can see that:\n",
    "\n",
    "$$\n",
    "log \\left ( \\frac{p_{t}}{p_{t-1}} \\right ) = - log \\left ( \\frac{p_{t-1}}{p_{t}} \\right )\n",
    "$$\n",
    "\n",
    "By using the numbers of the above example, we can see that the log returns from 100 to 120 are equal (but negative sign) with the log returns of 120 to 100.\n",
    "\n",
    "**Exercise 3:**\n",
    "\n",
    "Create a custom function in Python that, given a price array (for example `asset_prices['MSFT']`), will return the array of the logarithmic returns.\n",
    "\n",
    "\n",
    "As we have illustrated above, we can only distinguish between the two kinds of returns in terms of use cases. But now we need to be a bit more specific as to when and how. In order to do so, we need to do one observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 566,
     "status": "ok",
     "timestamp": 1732130381030,
     "user": {
      "displayName": "Christos Koutkos",
      "userId": "05788829401179389824"
     },
     "user_tz": -120
    },
    "id": "OyCypFeyVBgi",
    "outputId": "e07a6621-5761-4cdb-effe-43f36ea80204"
   },
   "outputs": [],
   "source": [
    "print(\"The 25th quantile is: \", r['MSFT'].quantile(0.25),\"\\nThe 75th quantile is: \", r['MSFT'].quantile(0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLfXADsEWeCL"
   },
   "source": [
    "The above results illustrate that, on many occasions, the daily returns are of the order of $1\\%$. Additionally, for the logarithmic and the percentage returns, it is true that:\n",
    "$$\n",
    "r_{\\text{log}} = ln \\left ( \\frac{p_{t}}{p_{t-1}} \\right ) =\n",
    "ln \\left ( \\frac{p_{t} - p_{t-1} + p_{t-1}}{p_{t-1}} \\right ) =\n",
    "ln (1 + r_{\\text{pct}})\n",
    "$$\n",
    "\n",
    "By using the Taylor expansion on the formula above, we get:\n",
    "$$\n",
    "r_{\\text{log}} = ln (1 + r_{\\text{pct}}) = r_{\\text{pct}} - \\frac{r_{\\text{pct}}^2}{2} + \\frac{r_{\\text{pct}}^3}{3} - \\frac{r_{\\text{pct}}^4}{4} + \\dots\n",
    "$$\n",
    "\n",
    "Since we observed that the daily returns are small $< 0.01$, we can assume that the higher terms ($r_{\\text{pct}}^3$ and above ) will be very small:\n",
    "\n",
    "$$\n",
    "r_{\\text{log}} = r_{\\text{pct}} - \\frac{r_{\\text{pct}}^2}{2} + O(R^3)\n",
    "$$\n",
    "\n",
    "By taking the expectation of both parts in the formula above and performing algebraic calculations, we have that:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(r_{\\text{log}}) \\approx \\mathbb{E}(r_{\\text{pct}}) - \\frac{\\sigma^2}{2}\n",
    "$$\n",
    "where $\\sigma$ is the variance of the percent returns.\n",
    "\n",
    "The above result illustrates two important facts:\n",
    "1. When volatility is low, we expect that the log returns and percent returns will be very close (Ruppert and Matteson).\n",
    "2. When the sampling frequency gets lower, the log and percent returns tend to get larger in absolute values, and thus, volatility tends to get bigger as well. This results in divergence between the two returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 2138,
     "status": "ok",
     "timestamp": 1732130383165,
     "user": {
      "displayName": "Christos Koutkos",
      "userId": "05788829401179389824"
     },
     "user_tz": -120
    },
    "id": "s70hkb5qNfA6",
    "outputId": "bf6bccf4-6376-4154-b9c9-125d7528b037"
   },
   "outputs": [],
   "source": [
    "# Figure 1\n",
    "\n",
    "pct_returns_msft = asset_prices['MSFT'].pct_change().dropna() # Calculating pct returns\n",
    "log_returns_msft = np.log(asset_prices['MSFT'] / asset_prices['MSFT'].shift(1)).dropna() # Calculating log returns\n",
    "\n",
    "pct_change_msft_roll_mean = pct_returns_msft.rolling(15).mean() # Rolling average of pct returns\n",
    "log_returns_msft_roll_mean = log_returns_msft.rolling(15).mean() # Rolling average of log returns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "ax.set_title('Difference between pct and log returns vs Price of MSFT')\n",
    "ax.plot(asset_prices['MSFT'], label = 'MSFT Price', alpha = 0.7)\n",
    "ax.set_ylabel('Price')\n",
    "ax.set_xlabel('Date')\n",
    "ax.legend()\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot((pct_change_msft_roll_mean - log_returns_msft_roll_mean), color = 'red', alpha = 0.5, label = 'Difference between pct and log returns')\n",
    "ax2.set_ylabel('Difference')\n",
    "ax2.legend(loc = (0.01,0.89))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Nmc1sRTc0c8"
   },
   "source": [
    "Figure 1 illustrates the difference between the pct and log returns. During low vol periods, both returns are very close, whereas during the March 2020 market crash, this difference becomes pronounced.\n",
    "\n",
    "**Exercise 4:**\n",
    "\n",
    "MSFT is a low-volatility stock. Recreate Figure 1 but now use TSLA in order to illustrate clearly in which periods the two returns diverge.\n",
    "\n",
    "**Exercise 5:**\n",
    "\n",
    "Argue why calculating the arithmetic weighted mean of log returns of different assets at a specific period in time will not give you the portfolio returns at that period.\n",
    "\n",
    "**Exercise 6:**\n",
    "\n",
    "Using the provided `asset_prices` DataFrame, calculate the daily portfolio value by computing the weighted sum of the asset prices, using predetermined asset weights (as you would when calculating portfolio returns). Next, calculate the logarithmic returns of this portfolio value series. Discuss the potential use cases and advantages of using log returns in this context.\n",
    "\n",
    "**Exercise 7:**\n",
    "\n",
    "Using the provided `asset_prices` DataFrame, calculate the cumulative return of each asset over the entire period using three different methods:\n",
    "\n",
    "1. By using the daily percent returns array of each asset (hint: you will need to use the `prod` function).\n",
    "2. By using the daily log returns.\n",
    "3. By using the asset prices array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qsa6b9CwNVNi"
   },
   "source": [
    "## **3. Comparing Geometric and Arithmetic Returns**\n",
    "\n",
    "We will continue our discussion from Module 2 on how to annualize returns, but before delving into this subject, we will check a trivial example that illustrates the mean that should be used for each type of return, **if the compounding effect of returns is required**.\n",
    "\n",
    "### **3.1 Geometric Mean Example**\n",
    "\n",
    "Let's assume that at time $t_{0}$, you have 100 units. Between $t_{0}$ and $t_{1}$, your return was 20%, and between $t_{1}$ and $t_{2}$, it was -18%. We assume that $t_{i}$ are spaced evenly.\n",
    "\n",
    "**Calculating the Arithmetic Mean**\n",
    "\n",
    "The arithmetic mean of the returns is:\n",
    "\n",
    "$$\n",
    "r_{\\text{mean}} = \\frac{0.2 - 0.18}{2} = 0.01\n",
    "$$\n",
    "\n",
    "This suggests that, on average, the return for each period is +1%. Following this logic, in the first period, we would have earned:\n",
    "\n",
    "$$\n",
    "100 \\cdot 0.01 = 1 \\text{ unit }\n",
    "$$\n",
    "\n",
    "So, at the end of the first period, we would have 101 units. During the second period, assuming the same +1% return:\n",
    "\n",
    "$$\n",
    "101 \\cdot 0.01 = 1.01 \\text{ units }\n",
    "$$\n",
    "\n",
    "This implies that at the end of the second period, we would have:\n",
    "\n",
    "$$\n",
    "100 + 2.01 = 102.01 \\text{ units in total.}\n",
    "$$\n",
    "\n",
    "**Using the Actual Returns**\n",
    "\n",
    "Now, let’s follow a different approach by applying the actual returns sequentially.\n",
    "\n",
    "1. At the end of $t_{1}$, with a 20% return on 100 units:\n",
    "\n",
    "$$\n",
    "100 \\cdot 0.2 = 20 \\text{ units earned.}\n",
    "$$\n",
    "\n",
    "   So, you have 120 units at the end of $t_{1}$.\n",
    "\n",
    "2. During the second period, from $t_{1}$ to $t_{2}$, the return is -18%. Starting with 120 units:\n",
    "\n",
    "$$\n",
    "120 \\times 0.18 = 21.6 \\text{ units lost.}\n",
    "$$\n",
    "\n",
    "   So, at the end of $t_{2}$, you end up with:\n",
    "\n",
    "$$\n",
    "120 - 21.6 = 98.4 \\text{ units.}\n",
    "$$\n",
    "\n",
    "**Which Method Is Correct?**\n",
    "\n",
    "Let’s calculate the geometric mean return to see which method aligns with reality:\n",
    "\n",
    "$$\n",
    "r_{\\text{GeomMean}} = \\left(\\prod _{i=1}^{n} (1 + r_{i}) \\right)^{\\frac {1}{n}} - 1\n",
    "$$\n",
    "\n",
    "For our two periods:\n",
    "\n",
    "$$\n",
    "r_{\\text{GeomMean}} = \\left( (1 + 0.2) \\cdot (1 - 0.18) \\right)^{\\frac{1}{2}} - 1 = \\sqrt{1.2 \\cdot 0.82} - 1 \\approx 0.992 - 1 = -0.008\n",
    "$$\n",
    "\n",
    "Now let’s test this geometric mean to see if it reflects the actual ending value:\n",
    "\n",
    "$$\n",
    "100 \\cdot (1 - 0.008) \\cdot (1 - 0.008) \\approx 100 \\cdot 0.992 \\cdot 0.992 = 98.4 \\text{ units.}\n",
    "$$\n",
    "\n",
    "\n",
    "This example demonstrates the obvious: that the loss of 18% on 120 units is greater than the gain of 20% on 100 units, leading to an overall decrease in value. Therefore, despite the arithmetic mean suggesting a positive return of 1%, you actually lost money, as shown by the geometric mean. The geometric mean provides a more accurate representation of compounded returns over multiple periods, especially when returns are volatile.\n",
    "\n",
    "That is of course when we deal with one asset across time and not when we deal with many assets at a specific point.\n",
    "\n",
    "### **3.2 Arithmetic Mean**\n",
    "\n",
    "We have established that we cannot use the arithmetic mean with percent returns for one asset across time **if compounding is important for our analysis** (unless volatility is really low as we have explained). So the question is, can we use the arithmetic mean with the logarithmic returns? Let's rewrite the example above using the logarithmic returns instead of the simple ones.\n",
    "\n",
    "The log returns are:\n",
    "\n",
    "$$\n",
    "r_{\\text{log}_1} = \\log(1.2) \\approx 0.1823, \\quad r_{\\text{log}_2} = \\log(0.82) \\approx -0.1987\n",
    "$$\n",
    "\n",
    "Because log returns are additive, we can aggregate the returns as:\n",
    "\n",
    "$$\n",
    "r_{\\text{ArithMean}} = \\frac{1}{2} \\left ( r_{\\text{log}_1} + r_{\\text{log}_2} \\right ) =\\frac{1}{2} \\left ( \\log(1.2) + \\log(0.82) \\right )\n",
    "$$\n",
    "\n",
    "By the properties of logarithms:\n",
    "\n",
    "$$\n",
    "r_{\\text{ArithMean}} = \\frac{1}{2} \\log(1.2 \\cdot 0.82) \\approx \\log(0.984^{\\frac{1}{2}}) \\approx \\log(0.992)\n",
    "$$\n",
    "\n",
    "\n",
    "Exponentiating this gives the actual compounded return:\n",
    "\n",
    "$$\n",
    "\\exp(\\log(0.992)) = 0.992\n",
    "$$\n",
    "\n",
    "This corresponds to the geometric mean, which correctly reflects the final value:\n",
    "\n",
    "$$\n",
    "100 \\cdot 0.992 \\cdot 0.992 \\approx 98.4 \\text{ units.}\n",
    "$$\n",
    "\n",
    "Thus, the arithmetic mean $r_{\\text{ArithMean}}$ of log returns provides the *correct* average return, aligning with the actual compounding of returns.\n",
    "\n",
    "\n",
    "**Exercise 8:**\n",
    "\n",
    "By using the properties of the logarithm, show that the log returns are indeed continuously compounded returns. (Hint: $r^{t_{1}}_{\\text{log}} + r^{t_{2}}_{\\text{log}} + r^{t_{3}}_{\\text{log}} + \\dots = ln \\left ( \\frac{p_{t_{2}}}{p_{t_{1}}} \\right ) + ln \\left ( \\frac{p_{t_{3}}}{p_{t_{2}}} \\right ) + \\dots$). Show that:\n",
    "\n",
    "$$\n",
    "ln(r_{\\text{GeomMean}} + 1) = \\frac{1}{n}\\sum_{t=1}^{t=n} r^{t}_{\\text{log}}\n",
    "$$\n",
    "\n",
    "Let's verify Exercise 8 using real-world data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1732130383165,
     "user": {
      "displayName": "Christos Koutkos",
      "userId": "05788829401179389824"
     },
     "user_tz": -120
    },
    "id": "IsemxFk-3Wnb",
    "outputId": "6f8d553e-259c-4219-b505-11e813a6c013"
   },
   "outputs": [],
   "source": [
    "percent_returns_tsla = asset_prices['TSLA'].loc[\"2020-06-01\":\"2020-09-30\"].pct_change().dropna() # Calculating daily percent returns\n",
    "geom_mean_tsla_1 = ((1 + percent_returns_tsla).prod() ** (1/len(percent_returns_tsla))) - 1 # Calculating geometric mean\n",
    "\n",
    "log_returns_tsla = np.log(asset_prices['TSLA'].loc[\"2020-06-01\":\"2020-09-30\"] / asset_prices['TSLA'].loc[\"2020-06-01\":\"2020-09-30\"].shift(1)).dropna() # Calculating daily log returns\n",
    "arithmetic_mean_tsla_1 = log_returns_tsla.mean() # Calculating arithmetic mean\n",
    "\n",
    "print(\"Logarithm of geometric Mean of TSLA + 1: \", np.log(geom_mean_tsla_1 + 1))\n",
    "print(\"Arithmetic Mean of TSLA: \", arithmetic_mean_tsla_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Xyu6fUZru95"
   },
   "source": [
    "### **3.3 Choosing the Correct Mean for Percent Returns**\n",
    "\n",
    "When analyzing investment returns, it's essential to choose the ppropriate mean based on the context. The arithmetic mean is **the correct measure for calculating expected returns in statistical models and portfolio optimization**. It represents the average return per period without considering the compounding effect. This is crucial when forecasting future returns, calculating expected portfolio returns, and performing risk assessments, as it aligns with the linear properties required in these calculations.\n",
    "\n",
    "On the other hand, the geometric mean accurately reflects the average compounded growth rate over multiple periods. It accounts for the effects of compounding and is suitable for evaluating the historical performance of an investment over time. However, the geometric mean should not be used in place of the arithmetic mean when performing statistical analyses or modeling expected returns, as it does not provide an unbiased estimate of future performance.\n",
    "\n",
    "In summary, while the geometric mean is invaluable for understanding the actual growth of an investment due to compounding, the arithmetic mean remains the fundamental measure in statistical contexts and portfolio management when assessing expected returns and making investment decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waYQvHP7N3VS"
   },
   "source": [
    "## **4. Annualizing Percent Returns**\n",
    "\n",
    "The above (trivial in many cases) examples were used to create the intuition that you will need to distinguish between annualizing logarithmic and percent returns. Annualizing returns is crucial since it allows us to compare investments over different time periods on a common annual basis. This process is essential for evaluating performance and comparing different assets.\n",
    "\n",
    "For percent returns, we use the geometric mean to account for compounding effects. The formula for annualizing daily percent returns is:\n",
    "\n",
    "$$\n",
    "R_{\\text{Annual}} = (1 + r_{\\text{GeomMean}})^N - 1\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $R_{\\text{Annual}}$ is the annualized return\n",
    "* $N$ is the number of trading periods in a year (it could be 252 days or 12 months for stocks, or 365 days for continuously traded assets, etc.)\n",
    "\n",
    "## **5. Annualizing Logarithmic Returns**\n",
    "\n",
    "For logarithmic returns, the annualization process is simpler due to their additive property. The formula for annualizing daily log returns is:\n",
    "\n",
    "$$\n",
    "R_{\\text{Annual,Log}} = N \\cdot r_{\\text{ArithMean}}\n",
    "$$\n",
    "\n",
    "If we want to convert it back to the \"actual\" annual returns, then:\n",
    "\n",
    "$$\n",
    "R_{\\text{Annual}} = e^{N \\cdot r_{\\text{ArithMean}}} - 1\n",
    "$$\n",
    "\n",
    "Let's calculate the annual returns of MSFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1732130383165,
     "user": {
      "displayName": "Christos Koutkos",
      "userId": "05788829401179389824"
     },
     "user_tz": -120
    },
    "id": "hRQtuRxMpRYg",
    "outputId": "ca274870-2605-47fc-ae9d-893b59f4f670"
   },
   "outputs": [],
   "source": [
    "geom_mean_msft = ((1 + pct_returns_msft).prod() ** (1/len(pct_returns_msft))) - 1 # Calculating geometric mean\n",
    "annual_pct_returns_msft = (1 + geom_mean_msft) ** 252 - 1 # Annualizing percent returns\n",
    "\n",
    "annual_log_returns_msft = log_returns_msft.mean() * 252 # Annualizing log returns\n",
    "\n",
    "print(\"Annualized pct returns of MSFT: \", annual_pct_returns_msft)\n",
    "print(\"Annualized log returns of MSFT: \", annual_log_returns_msft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrPqKRq4sUo3"
   },
   "source": [
    "**Exercise 9:**\n",
    "\n",
    "Calculate the annual returns for TSLA, both for percent and log returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flNhlo1juP77"
   },
   "source": [
    "## **6. Portfolio Variance**\n",
    "\n",
    "### **6.1 Overview of Portfolio Variance**\n",
    "\n",
    "This was discussed at a high level during the Financial Markets course, but here, we will show how to calculate variance in Python with empirical data. While returns are important, investors are also concerned about risk or volatility.  \n",
    "\n",
    "But let's think for a moment how we could measure risk for a portfolio of $n$ assets. If we had one asset, then we could calculate the standard deviation of returns and this could give us an approximation of how the returns deviate from the mean return. Similarly, one can argue that we can calculate the portfolio returns and then the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1732130383166,
     "user": {
      "displayName": "Christos Koutkos",
      "userId": "05788829401179389824"
     },
     "user_tz": -120
    },
    "id": "n6LK2iCNAa0A",
    "outputId": "c5218af8-03e1-4ab4-c30b-cc27e5f0a1c5"
   },
   "outputs": [],
   "source": [
    "r_port.std() # Calculating standard deviation of portfolio returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLKLVG50AUVZ"
   },
   "source": [
    "In Financial Markets, we also expressed portfolio variance as:\n",
    "\n",
    "$$\n",
    "\\sigma^{2}_{p} = \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_i w_j \\text{Cov}(r_i, r_j)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $w_i$ = the portfolio weight of the $i$th asset  \n",
    "* $\\text{Cov}_{i,j}$ = the covariance of two assets, which can be expressed as $\\rho_{_{(i,j)}} \\sigma_i \\sigma_j$, where $\\rho_{_{(i,j)}}$ is the correlation coefficient between the two assets\n",
    "\n",
    "\n",
    "The formula below is equivalent to the one above, but it is given in matrix notation:\n",
    "\n",
    "$$\n",
    "\\sigma^{2}_{p} = \\mathbf{w}^{T} \\cdot \\Sigma \\cdot \\mathbf{w}\n",
    "$$\n",
    "\n",
    "Where $\\Sigma$ is the portfolio covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1732130383166,
     "user": {
      "displayName": "Christos Koutkos",
      "userId": "05788829401179389824"
     },
     "user_tz": -120
    },
    "id": "B-x4L1rQDN8e",
    "outputId": "5eedcc65-90de-46f1-b718-238df84af30b"
   },
   "outputs": [],
   "source": [
    "port_var = w.T @ r.cov() @ w # Calculating portfolio variance\n",
    "port_std = np.sqrt(port_var) # Calculating portfolio standard deviation\n",
    "\n",
    "print(\"Portfolio variance: \", port_var)\n",
    "print(\"Portfolio standard deviation: \", port_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EM7c8cKDhUH8"
   },
   "source": [
    "In the above cell, we have showcased that the intuitive and straightforward way of calculating portfolio variance (by computing std directly on the portfolio value) can be replicated by using the covariance matrix and the weights. What we have essentially done is decompose the portfolio variance into several components:\n",
    "* The weights\n",
    "* The individual variances\n",
    "* The between pairs covariances\n",
    "\n",
    "The above result is somewhat anticipated. If the variance of an asset suddenly changes, the portfolio variance should change. If the weight of a high variance asset changes, then we expect that the portfolio variance will change accordingly. But what about covariances?\n",
    "\n",
    "In order to see more clearly, we will decompose the covariance matrix a bit more:\n",
    "\n",
    "$$\n",
    "\\Sigma = D \\cdot R \\cdot D\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* D is a diagonal matrix with the standard deviations of each asset\n",
    "* R is the correlation matrix\n",
    "\n",
    "The last decomposition shows us that the portfolio variance depends on the pairwise (Pearson) correlations as well.\n",
    "\n",
    "### **6.2 Impact of Correlation on Portfolio Variance**\n",
    "\n",
    "Let's write the portfolio variance in its quadratic form but incorporating correlations instead of covariances:\n",
    "\n",
    "$$\n",
    "\\sigma_p^2 = \\sum_{i=1}^{n} w_i^2 \\sigma_i^2 + \\sum_{i=1}^{n}\\sum_{j \\neq i}^{n} w_i w_j \\sigma_{ij}\n",
    "$$\n",
    "\n",
    "By substituting $\\sigma_{ij} = \\rho_{ij} \\sigma_i \\sigma_j$, we have:\n",
    "\n",
    "$$\n",
    "\\sigma_p^2 = \\sum_{i=1}^{n} w_i^2 \\sigma_i^2 + \\sum_{i=1}^{n}\\sum_{j \\neq i}^{n} w_i w_j \\rho_{ij} \\sigma_i \\sigma_j\n",
    "$$\n",
    "\n",
    "\n",
    "To understand how the portfolio variance changes with respect to the correlation $\\rho_{ij}$, we will take the partial derivative of $\\sigma_p^2$ with respect to $\\rho_{ij}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma_p^2}{\\partial \\rho_{ij}} = \\frac{\\partial}{\\partial \\rho_{ij}} \\left( \\sum_{i=1}^{n} w_i^2 \\sigma_i^2 + \\sum_{i=1}^{n}\\sum_{j \\neq i}^{n} w_i w_j \\rho_{ij} \\sigma_i \\sigma_j \\right)\n",
    "$$\n",
    "\n",
    "The first sum is independent of $\\rho_{ij}$, so its derivative with respect to $\\rho_{ij}$ is zero. The derivative of the second sum, with respect to $\\rho_{ij}$, gives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma_p^2}{\\partial \\rho_{ij}} = w_i w_j \\sigma_i \\sigma_j\n",
    "$$\n",
    "\n",
    "We have already assumed that $w_{i} > 0$ and we know that standard deviations are always positive. That means that:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sigma_p^2}{\\partial \\rho_{ij}} > 0\n",
    "$$\n",
    "\n",
    "Keeping all else the same, if the correlations between two assets increases/decreases, the portfolio variance will increase/decrease. Subsequently, if the correlations of many assets increase at the same time, the portfolio variance will increase faster, and that could be an indication of systemic risk.\n",
    "\n",
    "Let's showcase some of the above facts with real-world data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "executionInfo": {
     "elapsed": 1942,
     "status": "ok",
     "timestamp": 1732130385102,
     "user": {
      "displayName": "Christos Koutkos",
      "userId": "05788829401179389824"
     },
     "user_tz": -120
    },
    "id": "rMnRGZ8z6-CL",
    "outputId": "ce1abdae-be02-45bb-8f99-2c0bbc36d967"
   },
   "outputs": [],
   "source": [
    "# Figure 2\n",
    "# We will visualize the asset's std and returns vs portfolio\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (14,4))\n",
    "\n",
    "ax1.bar(x = r.columns, height = r.std(), alpha = 0.7)\n",
    "ax1.hlines(y = port_std, xmin = -0.4, xmax = 4.4, linestyle = \"--\", color = 'red', alpha = 0.8, label = \"Portfolio Std\")\n",
    "ax1.set_title(\"Assets vs Portfolio Volatlity\")\n",
    "ax1.set_ylabel(\"Volatility\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.bar(x = r.columns, height = (r + 1).prod() - 1, alpha = 0.7) # Make sure you can explain why the `(r + 1).prod() - 1` is the total return\n",
    "ax2.hlines(y = (r_port + 1).prod() - 1, xmin = -0.4, xmax = 4.4, linestyle = \"--\", color = 'red', alpha = 0.8, label = \"Portfolio Total Returns\")\n",
    "ax2.set_title(\"Assets vs Portfolio Total Returns\")\n",
    "ax2.set_ylabel(\"Total Returns (* 100%)\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNpqJ6AKV9QF"
   },
   "source": [
    "In Figure 2, the weights we selected (randomly) gave us a portfolio whose volatility is close to that of the majority of the assets, but its returns are well above.\n",
    "\n",
    "Let's test the relationship of the portfolio variance with the one of a random asset, for example MSFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "executionInfo": {
     "elapsed": 3758,
     "status": "ok",
     "timestamp": 1732130388856,
     "user": {
      "displayName": "Christos Koutkos",
      "userId": "05788829401179389824"
     },
     "user_tz": -120
    },
    "id": "JhCfy70Seotm",
    "outputId": "aa2e8d66-9961-4c33-aeb2-05cdd9b7babc"
   },
   "outputs": [],
   "source": [
    "# Figure 3\n",
    "multipliers = np.linspace(0.1,5,100) # Creating an array that will use to multiply the MSFT returns array.\n",
    "portfolio_variances = []\n",
    "msft_variances = []\n",
    "\n",
    "for multiplier in multipliers:\n",
    "  temp_returns = r * np.array([1, 1, 1, multiplier, 1]) # Multiplying MSFT returns with a number in order to scale the returns linearly, changing thus the MSFT variance.\n",
    "  temp_port_variance = w.T @ temp_returns.cov() @ w # Calculating portfolio variance\n",
    "  msft_variances.append((temp_returns['MSFT'] * multiplier).var())\n",
    "  portfolio_variances.append(temp_port_variance)\n",
    "  assert np.allclose(r.corr(), temp_returns.corr()) # For every new array of returns where MSFT returns are multiplied with a number, the correlations remain the same\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (16,5))\n",
    "\n",
    "ax1.plot(multipliers, portfolio_variances)\n",
    "ax1.scatter(1, port_var, color = 'red', label = \"Initial Portfolio Variance (non scaled MSFT returns)\")\n",
    "ax1.set_title(\"Portfolio Variance as MSFT variance is increasing\")\n",
    "ax1.set_xlabel(\"Multiplier\")\n",
    "ax1.set_ylabel(\"Variance\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(multipliers[10:30], portfolio_variances[10:30])\n",
    "ax2.scatter(1, port_var, color = 'red', label = \"Initial Portfolio Variance (non scaled MSFT returns)\")\n",
    "ax2.plot(multipliers[10:30], msft_variances[10:30], color = 'purple', label = \"MSFT Variance (non scaled MSFT returns)\")\n",
    "ax2.set_title(\"Portfolio Variance vs MSFT variance\")\n",
    "ax2.set_xlabel(\"Multiplier\")\n",
    "ax2.set_ylabel(\"Variance\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7FzqxS2iBW4"
   },
   "source": [
    "Figure 3 illustrates the effects of the increase of the variance of one asset (MSFT in our case) on the variance of the portfolio. As we expect, the relationship of the portfolio variance and the variance of any asset is quadratic (left graph). The right graph illustrates the difference in risk of holding only one asset vs. a portfolio.\n",
    "\n",
    "Let's now do the same but for correlations. For simplicity reasons, we will not simulate correlated assets (as you have already seen in Module 2 by using the Cholesky Decomposition), but we will merely create synthetic correlation matrices to show the effects of changing correlations in the portfolio variance.\n",
    "\n",
    "In the next example, we keep variances and weights constant. We assume 100 assets for which we construct 18 correlation matrices. The 1st corr matrix is allowed to take values from -1 to 1. For each subsequent corr matrix, the low boundary changes as follows $-1 + 0.1 \\cdot j$. That means that the 2nd corr matrix will have correlations from -0.9 to 1 and the 18th corr matrix from 0.8 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "executionInfo": {
     "elapsed": 892,
     "status": "ok",
     "timestamp": 1732130389746,
     "user": {
      "displayName": "Christos Koutkos",
      "userId": "05788829401179389824"
     },
     "user_tz": -120
    },
    "id": "Ik5BizOqhbtK",
    "outputId": "5cee54b8-5538-44ec-b3b6-dc8cd6f13398"
   },
   "outputs": [],
   "source": [
    "n_assets = 100\n",
    "\n",
    "w = np.random.dirichlet(np.ones(n_assets), size=1)[0] # Simulate weights using Dirichlet distribution\n",
    "\n",
    "degrees_of_freedom = 8  # Adjust the skewness of the Chi-square distribution\n",
    "variances = np.random.chisquare(df=degrees_of_freedom, size=n_assets)\n",
    "variances = variances / np.max(variances) * 0.003  # Scale to max variance of 0.003\n",
    "std_devs = np.sqrt(variances)\n",
    "D = np.diag(std_devs)\n",
    "\n",
    "# Construct the correlation matrices\n",
    "corr_matrices = []\n",
    "for j in range(19):\n",
    "  R = np.zeros((n_assets, n_assets))\n",
    "  low_boundary = -1 + j*0.1\n",
    "  for i in range(1, n_assets):\n",
    "      R[i, :i] = np.random.uniform(low_boundary, 1, i) # Creating the lower triangular part of the correlation matrix\n",
    "\n",
    "  R = R + R.T # Comment this line yourself\n",
    "  np.fill_diagonal(R, 1)\n",
    "  corr_matrices.append(R)\n",
    "\n",
    "# Calculate portfolio variances\n",
    "portfolio_variances = []\n",
    "for R in corr_matrices:\n",
    "  portfolio_variance = w.T @ D @ R @ D @ w\n",
    "  portfolio_variances.append(portfolio_variance)\n",
    "\n",
    "# Plot portfolio variance against correlations\n",
    "x_labels = [str((round(-1 + j*0.1, 1), 1)) for j in range(19)]\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(portfolio_variances)\n",
    "plt.title(\"Portfolio Variance vs Correlation\")\n",
    "plt.xlabel(\"Correlation Interval\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.xticks(ticks = range(19), labels = x_labels, rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTvgk5xqGbe0"
   },
   "source": [
    "Students who do not have a finance background may not have expected that result. Even when the individual variances and the weights remain constant, changes in correlations affect the portfolio variance. Specifically in our example, the portfolio variance is the lowest when the correlations were sampled from `uniform` distribution and interval $[-1,1]$. As the assets became more correlated, the variance went up as well.\n",
    "\n",
    "**Exercise 10:**\n",
    "\n",
    "Select $n \\geq 2$ stocks/bonds and construct a portfolio (select appropriate weights) using historical data such that the portfolio variance is $0$ (or really close to $0$ in order to be considered $0$) and the returns are positive (far from $0$). Present your findings in the forum. If you did not find one, present the reasons you could not find one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svvLY-cEMmzm"
   },
   "source": [
    "## **7. Portfolio Weights**\n",
    "\n",
    "Assume we can construct a portfolio that consists of stocks, bonds, and crypto. An investor might choose to hold a lot of bonds while another chooses mostly stocks and a third one loves crypto. The choice has to do with the risk profile of the investor; in other words, the choice of weights will satisfy the needs for specific returns given a risk tolerance (variance).\n",
    "\n",
    "From this perspective, the choice of weights is a subjective matter, and indeed in practice, investors behave very differently (Elton et al.). One of the best ways to explore this is by constructing the Efficient Frontier graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "executionInfo": {
     "elapsed": 12624,
     "status": "ok",
     "timestamp": 1732130402363,
     "user": {
      "displayName": "Christos Koutkos",
      "userId": "05788829401179389824"
     },
     "user_tz": -120
    },
    "id": "mJrcBlag0F8Z",
    "outputId": "8f04334e-c3a2-4c6e-b739-26559d8e54e3"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "weights = np.random.dirichlet(np.ones(5)*0.7, size = 20000) # Creating 20000 sets of weights using dirichlet distribution\n",
    "\n",
    "assert np.isclose(np.sum(weights, axis = 1), 1).all() # Check that each set of weights sum up to 1\n",
    "\n",
    "eff_front_dict = {}\n",
    "cov_matrix_ret = r.cov() * 252\n",
    "expected_returns = r.mean() * 252\n",
    "\n",
    "# Filling the eff_front_dict\n",
    "for w in weights:\n",
    "  port_ret = expected_returns @ w.T # Annualized percent returns as expected returns\n",
    "  port_std = np.sqrt(w.T @ cov_matrix_ret @ w)\n",
    "  eff_front_dict[str(list(w))] = [port_ret, port_std]\n",
    "\n",
    "eff_frontier_dataframe = pd.DataFrame(eff_front_dict, index = ['Returns', 'Standard Deviation']).T # Storing everything in one dataframe\n",
    "\n",
    "def get_portfolio_stats(weights, expected_returns, cov_matrix):\n",
    "    port_ret = expected_returns @ weights\n",
    "    port_std = np.sqrt(weights.T @ cov_matrix @ weights)\n",
    "    return port_ret, port_std\n",
    "\n",
    "def negative_sharpe_ratio(weights, expected_returns, cov_matrix, risk_free_rate=0.02):\n",
    "    port_ret, port_std = get_portfolio_stats(weights, expected_returns, cov_matrix)\n",
    "    sharpe_ratio = (port_ret - risk_free_rate) / port_std\n",
    "    return -sharpe_ratio\n",
    "\n",
    "def minimum_variance(weights, expected_returns, cov_matrix):\n",
    "    return get_portfolio_stats(weights, expected_returns, cov_matrix)[1]\n",
    "\n",
    "def efficient_frontier_point(expected_returns, cov_matrix, target_return):\n",
    "    n_assets = len(expected_returns)\n",
    "    args = (expected_returns, cov_matrix)\n",
    "    constraints = (\n",
    "        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1},\n",
    "        {'type': 'eq', 'fun': lambda w: get_portfolio_stats(w, expected_returns, cov_matrix)[0] - target_return}\n",
    "    )\n",
    "    bounds = tuple((0, 1) for _ in range(n_assets))\n",
    "\n",
    "    result = minimize(\n",
    "        minimum_variance,\n",
    "        x0=np.ones(n_assets) / n_assets,\n",
    "        args=args,\n",
    "        method='SLSQP',\n",
    "        bounds=bounds,\n",
    "        constraints=constraints\n",
    "    )\n",
    "\n",
    "    return result.x\n",
    "\n",
    "def get_efficient_frontier(expected_returns, cov_matrix, n_points=100):\n",
    "    # Find the minimum variance portfolio\n",
    "    n_assets = len(expected_returns)\n",
    "    args = (expected_returns, cov_matrix)\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    bounds = tuple((0, 1) for _ in range(n_assets))\n",
    "\n",
    "    min_var_result = minimize(\n",
    "        minimum_variance,\n",
    "        x0=np.ones(n_assets) / n_assets,\n",
    "        args=args,\n",
    "        method='SLSQP',\n",
    "        bounds=bounds,\n",
    "        constraints=constraints\n",
    "    )\n",
    "    min_ret, min_std = get_portfolio_stats(min_var_result.x, expected_returns, cov_matrix)\n",
    "\n",
    "    # Find the maximum return portfolio\n",
    "    max_ret_idx = np.argmax(expected_returns)\n",
    "    max_ret = expected_returns[max_ret_idx]\n",
    "\n",
    "    # Generate points on the efficient frontier\n",
    "    target_returns = np.linspace(min_ret, max_ret, n_points)\n",
    "    efficient_portfolios = []\n",
    "\n",
    "    for target_return in target_returns:\n",
    "        weights = efficient_frontier_point(expected_returns, cov_matrix, target_return)\n",
    "        ret, std = get_portfolio_stats(weights, expected_returns, cov_matrix)\n",
    "        efficient_portfolios.append([std, ret])\n",
    "\n",
    "    return np.array(efficient_portfolios)\n",
    "\n",
    "# Calculate the efficient frontier points\n",
    "efficient_points = get_efficient_frontier(expected_returns, cov_matrix_ret)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x=eff_frontier_dataframe['Standard Deviation'],\n",
    "           y=eff_frontier_dataframe['Returns'],\n",
    "           alpha=0.4)\n",
    "plt.scatter(x=r.std() * np.sqrt(252),\n",
    "           y=expected_returns,\n",
    "           color='red',\n",
    "           label=\"Individual Assets\",\n",
    "           alpha=0.7)\n",
    "plt.plot(efficient_points[:,0],\n",
    "         efficient_points[:,1],\n",
    "         'orange',\n",
    "         linewidth=3,\n",
    "         label='Efficient Frontier',\n",
    "         alpha=0.6)\n",
    "plt.title(\"Efficient Frontier\")\n",
    "plt.xlabel(\"Annual Portfolio Standard Deviation\")\n",
    "plt.ylabel(\"Annual Portfolio Returns\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKE3OvSkJAi7"
   },
   "source": [
    "Let's read the above illustration, which admittedly is one of the most important in portfolio management.\n",
    "\n",
    "First of all, we need to observe the shape of the frontier: the curve-like line that spans the left and upper part of the scatter plot represents all possible optimal portfolios that offer the highest expected return for a given level of risk (or the lowest risk for a given level of expected return). This curve is called the **efficient frontier**.\n",
    "\n",
    "Secondly, if we were to draw vertical lines on this graph, each line would intersect the frontier at a point representing the portfolio with the highest return for that specific level of risk. At the same time, there are plenty of weights that expose the portfolio to the same level of risk but for less return. That should bring our attention to the fact that even though the weights are subjective, if the portfolio that an investor chose does not lie in the efficient frontier, then the investor can do better in terms of returns by following the vertical line upwards until reaching the efficient frontier. In short, for any given level of risk (variance), there is an optimal portfolio composition that maximizes returns.\n",
    "\n",
    "The same as above is true if we were to draw horizontal lines. In this case, we can claim that for any given level of returns, we can minimize risk by following the horizontal line to the left up until it crosses the efficient frontier.\n",
    "\n",
    "As we move up and to the right along the frontier, we see higher potential returns but also higher risk. This illustrates the fundamental principle that to achieve higher returns, one generally must accept higher risk. The concave shape of the frontier illustrates the benefits of diversification. Portfolios on the frontier are typically well diversified, offering better risk-return profiles than individual assets.\n",
    "\n",
    "**Exercise 11:**\n",
    "\n",
    "Create your own portfolio, one to your liking: choose one or more equities, ETFs, bonds, and crypto. Try to include assets that are not that correlated with the rest of the portfolio (if this is possible) and also include some very high-volatility assets (large marketcap altcoins or newly issued ones). Use the code provided above in order to construct the efficient frontier. Paste the graph in the forums along with a **small** paragraph that lists the assets chosen.\n",
    "\n",
    "**Exercise 12 (Optional):**\n",
    "\n",
    "In the first paragraph of this notebook, we mentioned that given $n$ vectors $r_{i}$, in the absence of multicollinearity, it is true that $\\text{dim}(\\text{span}(r_{i})) = n$. Otherwise, the dimension of the $\\text{span}$ will be lower than $n$. This is a known result from linear algebra. Use your favorite linear algebra textbook in order to remember what multicollinearity is and then argue why an investor would keep an asset in their portfolio when that asset could be written as a perfect linear combination of the rest of the portfolio assets. Should the investor just omit such an asset? What is its contribution to lowering portfolio variance or increasing returns? Does it give flexibility in portfolio management?\n",
    "\n",
    "**Exercise 13:**\n",
    "\n",
    "Throughout this lesson, students were introduced to the correct usage of the arithmetic and geometric mean. In Section *2.4 Annualizing Returns*, we explained how to annualize the percent and log returns if the compounding effect was needed. Still, when constructing the efficient frontier, we did this: `expected_returns = r.mean() * 252`. Why? In your answer, consider the context of portfolio optimization and whether we are focusing on modeling expected returns for statistical purposes or calculating the actual historical growth of an investment due to compounding. Discuss the implications of using the arithmetic mean in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRo2wFOGtwFg"
   },
   "source": [
    "## **8. Conclusion**\n",
    "\n",
    "In this lesson, we learned that logarithmic returns are additive and useful for longer-term analysis, while percentage returns are more intuitive for short-term performance evaluation. We also discovered how to annualize returns, allowing for fair comparisons between investments with different time horizons.\n",
    "The lesson emphasized the importance of understanding portfolio variance and its components. We saw how individual asset variances, weights, and correlations all play crucial roles in determining overall portfolio risk. The efficient frontier concept illustrated the trade-off between risk and return, highlighting the benefits of diversification.\n",
    "\n",
    "Through practical examples and exercises using Python, we've gained hands-on experience in calculating and interpreting these metrics. This knowledge forms a crucial foundation for more advanced topics in portfolio management and financial analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKKS0keOFrc1"
   },
   "source": [
    "\n",
    "**References**\n",
    "\n",
    "* Elton, Edwin J., et al. *Modern Portfolio Theory and Investment Analysis*. John Wiley & Sons, 2009.\n",
    "\n",
    "* Ruppert, David, and David S. Matteson. *Statistics and Data Analysis for Financial Engineering*. 2nd ed., Springer, 2011."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KvTOJQ-GFrc1"
   },
   "source": [
    "---\n",
    "Copyright 2024 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "PRo2wFOGtwFg"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
