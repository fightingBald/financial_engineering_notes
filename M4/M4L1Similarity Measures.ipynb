{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "Keywords  关键词\tEuclidean Distance, Manhattan Distance, Cosine Similarity, Jaccard Index, Word Mover's Distance (WMD), Dynamic Time Warping, Pearson Correlation,\n",
    "欧氏距离、曼哈顿距离、余弦相似度、杰卡德指数、词移动距离 (WMD)、动态时间规整、皮尔逊相关系数\n",
    "\n",
    "Spearman Rank Correlation, Term Frequency-Inverse Document Frequency (TF-IDF), Text Analysis, Natural Language Processing (NLP), Information Retrieval\n",
    "斯皮尔曼等级相关、词频-逆文档频率 (TF-IDF)、文本分析、自然语言处理 (NLP)、信息检索\n",
    "'''"
   ],
   "id": "6be672baccc945a1"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-03T13:27:43.802128Z",
     "start_time": "2025-07-03T13:27:43.266739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T14:19:22.374982Z",
     "start_time": "2025-07-03T14:19:22.371100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np          # 导入 numpy —— 干向量运算的主力库\n",
    "\n",
    "# ----------- 模拟 5 天收益数据 -------------\n",
    "stock_a = np.array([ 0.02, -0.01,  0.03,  0.01, -0.02])   # 股票 A\n",
    "stock_b = np.array([ 0.03,  0.01,  0.02,  0.00, -0.01])   # 股票 B\n"
   ],
   "id": "cf6249eea7d6b096",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T14:19:39.910285Z",
     "start_time": "2025-07-03T14:19:39.906578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    " --------- Euclidean (L2) Distance 一段干啥？\n",
    "stock_a - stock_b 先算两个向量的差。\n",
    "** 2 把差平方；np.sum 所有维度加一起。\n",
    "np.sqrt 开根号得到直线距离。值越小 → 俩票涨跌幅度越像。\n",
    "----------\n",
    "'''\n",
    "euclidean_distance = np.sqrt(np.sum((stock_a - stock_b) ** 2))\n",
    "print(\"Euclidean Distance:\", euclidean_distance)\n"
   ],
   "id": "8c0d366309419298",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance: 0.0282842712474619\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "# --------- Manhattan (L1) Distance ----------\n",
    "先求差，再 np.abs 取绝对值，再 np.sum。\n",
    "相当于“走城市街区拐 90° 只走横竖”，距离越小 → 总体差异越小。对鬼一样的极端值没 L2 那么敏感。\n",
    "'''\n",
    "\n",
    "# --------- Manhattan (L1) Distance ----------\n",
    "manhattan_distance = np.sum(np.abs(stock_a - stock_b))\n",
    "print(\"Manhattan Distance:\", manhattan_distance)\n"
   ],
   "id": "af840e559deac94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --------- Cosine Similarity ----------\n",
    "# 这一段干啥？\n",
    "# np.dot 算内积，衡量「同向力度」。\n",
    "# np.linalg.norm 各自求长度。\n",
    "# 点积 / (长度乘积) 就是 cos θ。æ\n",
    "# 取值 [-1,1] —— 接近 1 = 两向量夹角小，常看“涨跌方向是否齐刷刷”。\n",
    "dot_product = np.dot(stock_a, stock_b)                  # 向量点积\n",
    "magnitude_a = np.linalg.norm(stock_a)                   # |A|\n",
    "magnitude_b = np.linalg.norm(stock_b)                   # |B|\n",
    "cosine_similarity = dot_product / (magnitude_a * magnitude_b)\n",
    "print(\"Cosine Similarity:\", cosine_similarity)\n"
   ],
   "id": "67577937640c0dbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "2.5 Simplified example of applying TF-IDF\n",
    "Scenario: Suppose we have a small collection of financial news headlines related to a company called \"Acme Corp.\":\n",
    "场景：假设我们有一些与“Acme Corp.”公司相关的财经新闻标题：\n",
    "\n",
    "Goal: We want to use TF-IDF to identify the most important words in each headline and understand the overall sentiment towards Acme Corp.\n",
    "目标：我们希望使用 TF-IDF 来识别每个标题中最重要的词语，并了解对 Acme Corp 的整体情绪。\n",
    "\n",
    "TF 就是“词在这篇里蹦几次”\n",
    "\n",
    "IDF 就是“全语料里它稀不稀有”\n",
    "\n",
    "TF-IDF 权重高＝这词在当前文章里牛X、在别处稀罕 —— 价值最大\n",
    "\n",
    "\n",
    "2. 掌握完整流水线\n",
    "练习把 从原始句子 → 数值矩阵 全流程过一遍：\n",
    "预处理/分词\n",
    "向量化\n",
    "抓词表\n",
    "查看权重\n",
    "以后遇见几十万条新闻，换源数据即可复用，省得每次瞎百度。\n",
    "\n",
    "3. 训练你的“读表”能力\n",
    "打印出 DataFrame 你能立刻看出：\n",
    "哪个标题关键词火力最猛（高权重）\n",
    "哪些标题关键词权重分布相似（主题接近）\n",
    "学会肉眼扫表，后面写聚类、情绪阀值才知道阈该调在哪。\n",
    "\n",
    "4. 铺垫后续进阶\n",
    "余弦相似度：拿 tfidf_matrix 直接 dot 就能做主题聚类\n",
    "情绪/分类模型：把 TF-IDF 当输入喂给逻辑回归、SVM\n",
    "风险雷达：固定风险词表，看它们权重时间序列\n",
    "没有这一步，你后面所有 NLP 金融因子都是空中楼阁。\n",
    "'''\n",
    "\n",
    "\n"
   ],
   "id": "667b2be82a2444bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T11:40:01.782962Z",
     "start_time": "2025-07-04T11:40:01.775696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "Step 1. Tokenization and Preprocessing:\n",
    "步骤 1.标记化和预处理：\n",
    "将每个标题分解成单个单词（标记）。\n",
    "删除停用词（常用词如“the”、“a”、“is”）和标点符号。\n",
    "将单词转换为小写。\n",
    "\n",
    "1. Acme Corp. announces record profits, stock surges.   ← 明显利好\n",
    "2. Acme Corp. faces regulatory scrutiny, shares decline.← 利空\n",
    "3. Market volatility impacts Acme Corp. earnings.        ← 中性∼负\n",
    "4. Acme Corp. expands into new markets, analysts optimistic. ← 偏多\n",
    "\n",
    "\n",
    "| 步 | 干啥             | 目的                      |\n",
    "| - | -------------- | ----------------------- |\n",
    "| 1 | Tokenize & 预处理 | 去废词、全小写，让向量更干净          |\n",
    "| 2 | 计算 TF、IDF      | 词在本条多？全语料稀？             |\n",
    "| 3 | 乘出 TF-IDF      | 得到“独特又高频”权重             |\n",
    "| 4 | 转 DataFrame    | 人眼一看就懂谁是大词              |\n",
    "| 5 | 单点取值           | 验证关键词，如 *profits* 权重大不大 |\n",
    "\n",
    "'''\n",
    "import numpy as np                                   # 数值计算\n",
    "import pandas as pd                                  # 打表漂亮\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # 核心 TF-IDF\n",
    "\n",
    "# ---------- 教材里的四条标题 ----------\n",
    "headlines = [\n",
    "    \"Acme Corp. announces record profits, stock surges.\",\n",
    "    \"Acme Corp. faces regulatory scrutiny, shares decline.\",\n",
    "    \"Market volatility impacts Acme Corp. earnings.\",\n",
    "    \"Acme Corp. expands into new markets, analysts optimistic.\"\n",
    "]\n",
    "\n",
    "\n"
   ],
   "id": "eb246ba08f3e4f7f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T11:41:28.337924Z",
     "start_time": "2025-07-04T11:41:28.323460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "让我们通过示例标题 1 来演示实施过程：“Acme Corp. 宣布创纪录的利润，股票飙升。”\n",
    "\n",
    "Tokenization and Preprocessing:\n",
    "标记化和预处理：\n",
    "\n",
    "Tokens: [\"acme\", \"corp\", \"announces\", \"record\", \"profits\", \"stock\", \"surges\"]\n",
    "令牌：[“acme”、“corp”、“announces”、“record”、“profits”、“stock”、“surges”]\n",
    "'''\n",
    "# ---------- 1) 创建 TF-IDF 向量器 ----------\n",
    "# 中文注释：停用常见英文废词，默认 tokenizer=英文分词\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# ---------- 2) 拟合语料（学习词表 + IDF） ----------\n",
    "vectorizer.fit(headlines)\n",
    "\n",
    "# ---------- 3) 文本 → TF-IDF 稀疏矩阵 ----------\n",
    "tfidf_matrix = vectorizer.transform(headlines)  # 行=标题，列=词\n",
    "\n",
    "# ---------- 4) 打印词表看看 ----------\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Vocabulary:\", feature_names)\n"
   ],
   "id": "4e26ca25642801a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['acme' 'analysts' 'announces' 'corp' 'decline' 'earnings' 'expands'\n",
      " 'faces' 'impacts' 'market' 'markets' 'new' 'optimistic' 'profits'\n",
      " 'record' 'regulatory' 'scrutiny' 'shares' 'stock' 'surges' 'volatility']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T11:42:06.390650Z",
     "start_time": "2025-07-04T11:42:06.373986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- 5) 把矩阵转成带行列标签的 DataFrame ----------\n",
    "labels = [f\"Headline{i+1}\" for i in range(len(headlines))]\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), index=labels, columns=feature_names)\n",
    "print(\"\\nTF-IDF 表格（保留三位小数）\")\n",
    "df.round(3)"
   ],
   "id": "44a514a6afe50a0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF 表格（保留三位小数）\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "            acme  analysts  announces   corp  decline  earnings  expands  \\\n",
       "Headline1  0.222     0.000      0.425  0.222    0.000     0.000    0.000   \n",
       "Headline2  0.222     0.000      0.000  0.222    0.425     0.000    0.000   \n",
       "Headline3  0.245     0.000      0.000  0.245    0.000     0.469    0.000   \n",
       "Headline4  0.222     0.425      0.000  0.222    0.000     0.000    0.425   \n",
       "\n",
       "           faces  impacts  market  ...    new  optimistic  profits  record  \\\n",
       "Headline1  0.000    0.000   0.000  ...  0.000       0.000    0.425   0.425   \n",
       "Headline2  0.425    0.000   0.000  ...  0.000       0.000    0.000   0.000   \n",
       "Headline3  0.000    0.469   0.469  ...  0.000       0.000    0.000   0.000   \n",
       "Headline4  0.000    0.000   0.000  ...  0.425       0.425    0.000   0.000   \n",
       "\n",
       "           regulatory  scrutiny  shares  stock  surges  volatility  \n",
       "Headline1       0.000     0.000   0.000  0.425   0.425       0.000  \n",
       "Headline2       0.425     0.425   0.425  0.000   0.000       0.000  \n",
       "Headline3       0.000     0.000   0.000  0.000   0.000       0.469  \n",
       "Headline4       0.000     0.000   0.000  0.000   0.000       0.000  \n",
       "\n",
       "[4 rows x 21 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acme</th>\n",
       "      <th>analysts</th>\n",
       "      <th>announces</th>\n",
       "      <th>corp</th>\n",
       "      <th>decline</th>\n",
       "      <th>earnings</th>\n",
       "      <th>expands</th>\n",
       "      <th>faces</th>\n",
       "      <th>impacts</th>\n",
       "      <th>market</th>\n",
       "      <th>...</th>\n",
       "      <th>new</th>\n",
       "      <th>optimistic</th>\n",
       "      <th>profits</th>\n",
       "      <th>record</th>\n",
       "      <th>regulatory</th>\n",
       "      <th>scrutiny</th>\n",
       "      <th>shares</th>\n",
       "      <th>stock</th>\n",
       "      <th>surges</th>\n",
       "      <th>volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Headline1</th>\n",
       "      <td>0.222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Headline2</th>\n",
       "      <td>0.222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Headline3</th>\n",
       "      <td>0.245</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Headline4</th>\n",
       "      <td>0.222</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 21 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T11:42:25.004190Z",
     "start_time": "2025-07-04T11:42:24.999757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "我们可以使用 feature_names 来访问特定的单词。当我们想要分析某个单词的 TF-IDF 得分时，可以使用 feature_names 列表查找其索引。以下代码重点介绍如何访问特定标题和单词的 TF-IDF 得分。此代码片段识别出我们感兴趣的特定标题（第一个标题）和单词（“profits”）。然后，它会在 TF-IDF 矩阵中找到相应的行和列，从矩阵中的该位置检索 TF-IDF 得分并打印得分\n",
    "'''\n",
    "# ---------- 6) 单独查看 'profits' 在第一条的权重 ----------\n",
    "row_idx = 0                                     # 第一条\n",
    "col_idx = feature_names.tolist().index('profits')\n",
    "score = tfidf_matrix[row_idx, col_idx]\n",
    "print(f\"\\n'profits' 的权重（Headline1）：{score:.4f}\")"
   ],
   "id": "e24bd5f3aa6dc7e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'profits' 的权重（Headline1）：0.4247\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "59bc470581d04481"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
